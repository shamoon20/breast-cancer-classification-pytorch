{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HvRXBjd1Mk2"
   },
   "source": [
    "# Deep Learning 2025 - Assignment 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auGAD4vQ1Mk5"
   },
   "source": [
    "## Outline\n",
    "#### <span style=\"color: blue;\">Submission</span>\n",
    "We will give 2 weeks for each assignment and more for the final project.\n",
    "\n",
    "The grades will be released after the deadline. Please also put your name and student number (if you have one) in the file name of the returned assignments. (DL2025_assign1_NAME_SURNAME_STUDENTNUMBER.ipynb)\n",
    "\n",
    "#### <span style=\"color: blue;\">In this assignment, you will learn</span>\n",
    "* How to load data and create a train/test split.\n",
    "* How to build your own Pytorch model for simple logistic regression (linear classification) problem.\n",
    "* Training the model with binary cross entropy loss in Pytorch.\n",
    "* Simple visualisation of data, loss and linear model.\n",
    "\n",
    "#### <span style=\"color: blue;\">Tasks & grading </span>(<span style=\"color:green\">10 points</span>)\n",
    "* **Part 1. Load dataset and split into training and testing sets (<span style=\"color:green\">1 point</span>)**\n",
    "* **Part 2. Linear classification and logistic regression (<span style=\"color:green\">9 points</span>)**\n",
    "  * Part 2.0 Sklearn Logistic Regression\n",
    "  * Part 2.1 Model (<span style=\"color:green\">3.5 points</span>)\n",
    "  * Part 2.2 Training (<span style=\"color:green\">5.5 points</span>)\n",
    "\n",
    "#### <span style=\"color: blue;\">Environment</span>\n",
    "Python 3 + Sklearn + Pytorch (>=1.3) + Pandas.<br>\n",
    "Other libraries should be installed correctly such as numpy, matplotlib, *etc*., according to the dependencies of the assignment. <br>\n",
    "If you haven’t installed Pytorch and Jupyter notebook in your computer, an alternative environment to finish the assignment is CSC notebook: [`https://noppe.2.rahtiapp.fi/`](https://noppe.2.rahtiapp.fi/) -> log in with HaKa using your university account -> Practical Deep Learning(refers to the following figure).\n",
    "Also, googlecolab is a very good option. Computer rooms TS135 and TS137 contain computers with GPUs that you can utilise as well.\n",
    "\n",
    "<img src=https://i.postimg.cc/RCy9SgHf/CSC-Jupyter.png width=\"1000\">\n",
    "\n",
    "We will give 2 weeks for each assignment and more for the final project. So it may happen where the new assignment is published before the deadline of the previous assignment. Please do not wait till the last minute to complete the assigments, as they can be time consuming.\n",
    "\n",
    "#### <span style=\"color: blue;\">Dataset</span>\n",
    "* Breast Cancer Wisconsin (Diagnostic) dataset\n",
    "  * It is a widely used dataset in machine learning and medical research\n",
    "particularly for binary classification tasks involving breast cancer diagnosis.\n",
    "  * It contains 569 samples each composed of 30 numerical features representing a breast tumor and a target class that is either benign (non-cancerous)or malignant (cancerous).\n",
    "  * It can be easily loaded from sklearn library.\n",
    "\n",
    "#### <span style=\"color: blue;\">Hints</span>\n",
    "* To find the place where you have to insert your solution, hit Crtl + F and search for **TODO:** . You are not supposed to modify the code from other parts.\n",
    "* **Be careful with the shape** of the weights, gradient, .. of your tensor in your implementation. Double check and make sure the shapes fit for computation, especially matrix multiplication.\n",
    "* Use only PyTorch functions in the model, and avoid explicit loops. For example, if you use a for loop inside the model function, you lose parallelism and the model will not run efficiently on GPUs. Once the data is loaded as tensors, you should continue to use PyTorch functions; converting them to NumPy arrays inside the training loop will prevent efficient training.\n",
    "* Check the examples from the Introduction to Deep Learning Software lecture, as they can help you better understand the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAM_J6DM1Mk6"
   },
   "source": [
    "## Part 1. Load dataset and split into training and testing sets\n",
    "\n",
    "In this part, we will load the Breast Cancer Wisconsin (Diagnostic) dataset from the sklearn library. The features will be stored in a pandas DataFrame, and the target class labels in a pandas Series. We will then normalize the feature values to have a mean of 0 and a standard deviation of 1. This normalization step is important because it helps the classification boundary converge more effectively during training.\n",
    "\n",
    "First, you need to split the samples into training and testing sets using the train_test_split function from sklearn, while preserving the distribution of class labels in both splits so that it is consistent with the overall dataset. Then, you will create a 2D plot of the first two features of the data, showing both the training and testing samples and distinguishing between the two different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47i2bU_G1Mk6"
   },
   "outputs": [],
   "source": [
    "# Run this cell as it is to ensure reproducibility\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# If using CUDA\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSm_JR8u1Mk7"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYgVuCyw3zVJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AH4vVe721Mk8"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO: (1 point, details are given below):\n",
    "\n",
    "# Divide the data into a training set (80%) and a test set (20%), resulting in X_train, X_test, y_train, and y_test (0.5 point)\n",
    "# Use the X_scaled array and y series during the division\n",
    "# Use train_test_split() function setting the random_state parameter to 42 and preserving the distribution of classes in y to be similar in both splits\n",
    "# Hint: search about stratify parameter in train_test_split() function\n",
    "# YOUR CODE HERE\n",
    "\n",
    "print(\"shape of X_train and y_train: \",X_train.shape,y_train.shape)\n",
    "print(\"shape of X_test and y_test: \",X_test.shape,y_test.shape)\n",
    "\n",
    "print(\"value_counts of y_train:\\n\", y_train.value_counts())\n",
    "print(\"value_counts of y_test:\\n\", y_test.value_counts())\n",
    "\n",
    "# plot the first 2 features in the dataset. (0.5 point)\n",
    "# Train and test samples should be in a single figure with two different markers and the samples of each class label in each split should have different colors.\n",
    "# Hint: plt.figure(), plt.scatter()\n",
    "\n",
    "first_2_features = data.feature_names[[0, 1]]\n",
    "X_train_first_2 = X_train[:, [0, 1]]\n",
    "X_test_first_2 = X_test[:, [0, 1]]\n",
    "\n",
    "print(\"Top 2 feature names:\", first_2_features.tolist())\n",
    "print(\"X_train_selected shape:\", X_train_first_2.shape)\n",
    "print(\"X_test_selected shape:\", X_test_first_2.shape)\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HI2EW-on1Mk8"
   },
   "source": [
    "## Part 2. Solving the linear classification problem in Pytorch using logistic regression algorithm\n",
    "Congratulations you have prepared the data correctly! Now we will move onto creating our model and training it.\n",
    "\n",
    "This task inculdes:\n",
    "\n",
    "\n",
    "*   Defining Models (Hints: torch.nn, nn.Module, nn.Parameter, torch.log, torch.tensor)\n",
    "*   Training Model (Hints: model.train, torch.optim.SGD, optimizer.zero_grad, loss.backward, optimizer.step)\n",
    "*   Evaluating Model (Hints: model.eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nct2CegD_34-"
   },
   "source": [
    "### 2.1 Sklearn Logistic Regression\n",
    "In this part, you will run the built-in logistic regression algorithm from the sklearn library to check the expected accuracy, which you should aim to achieve with your PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBMtNSPs-ngQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbQGfmzX1Mk8"
   },
   "source": [
    "### 2.2 Logistic Regregression Model (<span style=\"color:green\">3.5 points</span>)\n",
    "In this part, you will define your own logistic regression model class. To do that, you have to remember the following rules:\n",
    "1. The model class should be inherited from [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module);\n",
    "2. Re-write the **\\_\\_init\\_\\_** function and the **forward** function;\n",
    "3. In the **\\_\\_init\\_\\_** function, you should always call the parent's **\\_\\_init\\_\\_** function first.\n",
    "4. Don't use the nn.Linear() layer, implement it yourself as an independent model class.\n",
    "5. Use 1 tensor to define W (not 30) and 1 tensor to define b.\n",
    "6. Only torch functions and no iterations inside the model.\n",
    "7. Don't use the builtin Sigmoid or BCE loss implementations, implement them yourself as functions.\n",
    "\n",
    "This figure illustrates the different model components:\n",
    "<img src=https://i.postimg.cc/qv7QQ60G/Assignment-1-Arch.png width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPIBYOEP1Mk9"
   },
   "outputs": [],
   "source": [
    "# TODO (3.5 points, details are given below)\n",
    "import torch.nn as nn\n",
    "\n",
    "# define a custom Linear layer with W and b trainable parameters (1 point)\n",
    "# write the __init__ function (0.5 point) + write the forward function (0.5 point)\n",
    "# Hint: Search for torch.nn.Parameter\n",
    "class CustomLinear(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        # input_dim parameter represents the number of data features\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "\n",
    "# define a custom Sigmoid function (0.5 point)\n",
    "def custom_sigmoid(z):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "# define the Logistic Regression model class using the previously defined CustomLinear and custom_sigmoid (1.5 points)\n",
    "# define a class (0.5) + write the __init__ function (0.5 point) + write the forward function (0.5 points)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# Hint: the class name is  LogisticRegressionModel. It inherit from nn.Module\n",
    "\n",
    "\n",
    "# define a custom BCE Loss function (0.5 point)\n",
    "def custom_bce_loss(y_pred, y_true):\n",
    "    # Clamp predictions to avoid log(0)\n",
    "    y_pred = torch.clamp(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIeqgzZY1Mk9"
   },
   "source": [
    "### 2.3 Training (<span style=\"color:green\">5.5 points</span>)\n",
    "Here, you will train your model based on the training data and evaluate the model on testing data.\n",
    "1. Set proper number of iterations and learning rate.\n",
    "2. Remember to use a proper optimizer (you may have many choices: Adam, SGD, RMSprop, ... please find the detailed information in https://pytorch.org/docs/stable/optim.html and know how to use them).\n",
    "3. In order to train the model, use the custom BCE loss you implemented previously.\n",
    "4. The model must be trained only using training data.\n",
    "5. Remember to clear the old gradients of parameters before a new backward propagation.\n",
    "6. In every certain number of iterations, print the training loss, and the testing loss.\n",
    "7. Meanwhile, please track the training loss and the testing loss in each iteration. Once the training is done, the curves of losses should be plotted (two curves are drawn in the same figure, where x axis indicates iterations and y axis indicates the losses).\n",
    "8. Calculate the testing accuracy of the trained model and compare it to the one obtained from fitting the built-in implementation of sklearn.\n",
    "9. Lastly, plot the 2 most significant features in the classifcation process showing both the training and testing samples in addition to the decision boundary learned by W and b as if these are the only 2 features (use different showing styles to distinguish the samples based on the split and class label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFir7gHU1Mk9"
   },
   "outputs": [],
   "source": [
    "# TODO (5.5 points, details are given below)\n",
    "\n",
    "# Prepare data\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Make a instance of the model class LogisticRegressionModel (0.5 point)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# create an optimizer, set a workable learing rate (1 point)\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "training_losses = []\n",
    "testing_losses = []\n",
    "\n",
    "epochs = 4000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Train\n",
    "    # forward propagation\n",
    "    model.train() # change model to train mode\n",
    "\n",
    "    # calculate the output of model (0.5 point)\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    # calculate loss using you custom BCE implementation custom_bce_loss() above (0.5 point)\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    training_losses.append(loss.item())\n",
    "\n",
    "\n",
    "    # backward propagation (clear old gradients, calculate gradients of parameters using the loss, apply optimizer step) (1 point)\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    # calculate the testing loss (0.5 point)\n",
    "    model.eval() # change model to eval mode\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    testing_losses.append(test_loss.item())\n",
    "\n",
    "    # print intermediate losses for certain iterations\n",
    "    if epoch % 50 == 1:\n",
    "        print('Iteration: %04d | Training loss: %f | Testing loss: %f' % \\\n",
    "              (epoch, loss.data, test_loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvFpUMQG1Mk9"
   },
   "outputs": [],
   "source": [
    "# plot the traing losses and testing losses (0.5 point)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWGz5fLrMSm1"
   },
   "outputs": [],
   "source": [
    "# Calculate the testing loss and be sure that it is near the one obtained from sklearn builtin implementation\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor)\n",
    "    predicted_classes = (predictions > 0.5).float()\n",
    "    accuracy = (predicted_classes.eq(y_test_tensor).sum() / y_test_tensor.shape[0]).item()\n",
    "\n",
    "print(\"PyTorch Model Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3j1kZcILGrXU"
   },
   "source": [
    "To emphasize the concept of linear classification, we will visualize the decision boundary using a 2D plot that focuses on the two most influential features in the dataset. These features are identified based on the magnitudes of their corresponding linear weights after training—the two with the largest absolute weights are considered the most significant. Once identified, we will extract their values along with the associated weights, and use them to plot all sample points in the feature space. Finally, we will overlay the decision boundary to clearly demonstrate how the classifier separates the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeyxGDyFOJuO"
   },
   "outputs": [],
   "source": [
    "# Determine the most significant 2 features during classification as the ones having the largest corresponding linear weights magnitude after training\n",
    "# Detect these features and extract their values in both train and test samples\n",
    "\n",
    "# We assumed that the linear layer in the model class is named (linear) and the weights are named (W)\n",
    "# Modify this line if your variable names are different when defining the models classes\n",
    "weights = model.linear.W.detach().numpy().flatten()\n",
    "\n",
    "top2_indices = np.argsort(np.abs(weights))[-2:][::-1]\n",
    "top2_features = data.feature_names[top2_indices]\n",
    "\n",
    "# Extract the corresponding features from train and test sets\n",
    "X_train_selected = X_train[:, top2_indices]\n",
    "X_test_selected = X_test[:, top2_indices]\n",
    "\n",
    "print(\"Indices of top 2 features:\", top2_indices)\n",
    "print(\"Top 2 feature names:\", top2_features.tolist())\n",
    "print(\"X_train_selected shape:\", X_train_selected.shape)\n",
    "print(\"X_test_selected shape:\", X_test_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mpdkSTG-1Mk-"
   },
   "outputs": [],
   "source": [
    "# plot the most significant 2 features in the dataset. (0.5 point)\n",
    "# Train and test samples should be in a single figure with two different markers and the samples of each class label in each split should have different colors.\n",
    "# Add a line to the same plot representing the decision boundary as if these 2 features are the only ones using their 2 corresponding learned weights and the learned bias. (0.5 point)\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
